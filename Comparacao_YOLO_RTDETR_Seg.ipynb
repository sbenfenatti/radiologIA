{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063e9bc7",
   "metadata": {},
   "source": [
    "\n",
    "# üìò Compara√ß√£o de Segmenta√ß√£o: YOLOv12-seg (fallbacks) vs RT-DETR-Seg (Transformers)\n",
    "\n",
    "**Fluxo:** 1) GPU/ambiente ‚Üí 2) `CONFIG` ‚Üí 3) Montar Drive e validar datasets (YOLO/COCO) ‚Üí 4) Instalar libs e seeds ‚Üí 5) Treinar **YOLO-seg** (ordem: `yolo12l-seg.pt` ‚Üí `yolo11l-seg.pt` ‚Üí `yolov8l-seg.pt`) ‚Üí 6) Treinar **RT-DETR-Seg** no **COCO** (fallback para **Mask2Former**) ‚Üí 7) Avaliar (AP bbox/segm), salvar amostras ‚Üí 8) Comparar m√©tricas (tabela + CSV) ‚Üí 9) Visualiza√ß√µes lado a lado ‚Üí 10) Export√°veis (ONNX, .zip) ‚Üí 11) `report.json` ‚Üí 12) Notas/Ajustes.\n",
    "\n",
    "**Decis√µes de design:**\n",
    "- `IMG_SIZE=1024` por equil√≠brio entre detalhe e VRAM numa T4.\n",
    "- `rect=True` no YOLO preserva propor√ß√£o por batch.\n",
    "- RT-DETR-Seg via Transformers: tenta checkpoint *instance seg*; se indispon√≠vel, cai para **Mask2Former** mantendo a compara√ß√£o **transformer + mask head**.\n",
    "\n",
    "**Como mudar hiperpar√¢metros:** edite o bloco **CONFIG** (IMG_SIZE, EPOCHS, BATCH, MODEL_SIZE, CONF, IOU_THRES). Early stopping configurado nas chamadas de treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1) Verifica√ß√£o de GPU e ambiente (com detec√ß√£o de compatibilidade CUDA)\n",
    "import sys, subprocess, json, platform, os, re\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# GPU / driver\n",
    "nvsmi = \"\"\n",
    "try:\n",
    "    nvsmi = subprocess.check_output([\"nvidia-smi\"], text=True)\n",
    "    print(nvsmi)\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi not available or error:\", e)\n",
    "\n",
    "# Extrai vers√£o \"CUDA Version: 12.x\" do nvidia-smi, se poss√≠vel\n",
    "driver_cuda = None\n",
    "m = re.search(r\"CUDA Version:\\s*([0-9]+)\\.([0-9]+)\", nvsmi)\n",
    "if m:\n",
    "    driver_cuda = (int(m.group(1)), int(m.group(2)))  # (major, minor)\n",
    "\n",
    "# PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "        print(\"CUDA capability:\", torch.cuda.get_device_capability(0))\n",
    "        runtime = torch.version.cuda  # ex: '12.6'\n",
    "        print(\"Torch CUDA runtime:\", runtime)\n",
    "        rt = tuple(int(x) for x in runtime.split('.')) if runtime else None\n",
    "\n",
    "        # Se runtime (ex: 12.6) for maior que o suportado pelo driver (ex: 12.4), marcamos para usar cu124.\n",
    "        if driver_cuda and rt and (rt[0] == driver_cuda[0]) and (rt[1] > driver_cuda[1]):\n",
    "            os.environ[\"TORCH_NEED_CU124\"] = \"1\"\n",
    "            print(\">> Aviso: Driver mostra CUDA\", f\"{driver_cuda[0]}.{driver_cuda[1]}\",\n",
    "                  \"mas o PyTorch est√° com CUDA\", runtime, \"‚Üí rebaixar para cu124 na instala√ß√£o.\")\n",
    "        else:\n",
    "            os.environ[\"TORCH_NEED_CU124\"] = \"0\"\n",
    "    else:\n",
    "        # Sem CUDA? prossegue, mas n√£o vamos tentar cu124.\n",
    "        os.environ[\"TORCH_NEED_CU124\"] = \"0\"\n",
    "except Exception as e:\n",
    "    print(\"Torch info error:\", e)\n",
    "    os.environ[\"TORCH_NEED_CU124\"] = \"0\"\n",
    "\n",
    "# Snapshot r√°pido de libs j√° presentes\n",
    "def get_ver(module):\n",
    "    try:\n",
    "        m = __import__(module)\n",
    "        return getattr(m, \"__version__\", \"unknown\")\n",
    "    except:\n",
    "        return \"not installed\"\n",
    "\n",
    "print(json.dumps({\n",
    "    \"ultralytics\": get_ver(\"ultralytics\"),\n",
    "    \"transformers\": get_ver(\"transformers\"),\n",
    "    \"datasets\": get_ver(\"datasets\"),\n",
    "    \"evaluate\": get_ver(\"evaluate\"),\n",
    "    \"accelerate\": get_ver(\"accelerate\"),\n",
    "    \"pycocotools\": get_ver(\"pycocotools\"),\n",
    "    \"torchvision\": get_ver(\"torchvision\"),\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2) Bloco de CONFIG\n",
    "CONFIG = {\n",
    "    \"IMG_SIZE\": 1024,\n",
    "    \"EPOCHS\": 100,\n",
    "    \"BATCH\": 8,\n",
    "    \"SEED\": 42,\n",
    "    \"MODEL_SIZE\": \"l\",    # 'n','s','m','l','x'\n",
    "    \"CONF\": 0.25,\n",
    "    \"IOU_THRES\": 0.70,\n",
    "\n",
    "    # Paths do Drive\n",
    "    \"YOLO_DIR\": \"/content/drive/MyDrive/Dataset/YOLO_seg_original\",\n",
    "    \"COCO_DIR\": \"/content/drive/MyDrive/Dataset/COCO_original\",\n",
    "\n",
    "    # Sa√≠das no Drive\n",
    "    \"OUT_YOLO\": \"/content/drive/MyDrive/Experimentos/seg_yolo\",\n",
    "    \"OUT_RTDETR\": \"/content/drive/MyDrive/Experimentos/seg_transformer\",\n",
    "\n",
    "    # YOLO\n",
    "    \"YOLO_WORKERS\": 2,\n",
    "    \"YOLO_PATIENCE\": 20,\n",
    "\n",
    "    # Transformers\n",
    "    \"RT_USE_AMP\": True,\n",
    "    \"RT_LR\": 2e-5,\n",
    "    \"RT_WEIGHT_DECAY\": 1e-4,\n",
    "    \"RT_WARMUP_STEPS\": 200,\n",
    "    \"RT_GRAD_ACCUM\": 2,\n",
    "    \"RT_EVAL_CONF\": 0.05,\n",
    "}\n",
    "print(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3) Montar Drive e valida√ß√µes (YOLO + COCO) + corrigir data.yaml\n",
    "import os, re, glob, yaml, shutil, pathlib, json\n",
    "from google.colab import drive\n",
    "\n",
    "# Monta o Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "YOLO_DIR = CONFIG[\"YOLO_DIR\"]\n",
    "COCO_DIR = CONFIG[\"COCO_DIR\"]\n",
    "\n",
    "assert os.path.isdir(YOLO_DIR), f\"YOLO_DIR n√£o encontrado: {YOLO_DIR}\"\n",
    "assert os.path.isdir(COCO_DIR), f\"COCO_DIR n√£o encontrado: {COCO_DIR}\"\n",
    "\n",
    "def count_images_labels(split_dir):\n",
    "    img_dir = os.path.join(split_dir, \"images\")\n",
    "    lbl_dir = os.path.join(split_dir, \"labels\")\n",
    "    imgs = sorted(glob.glob(os.path.join(img_dir, \"**\", \"*.jpg\"), recursive=True) +\n",
    "                  glob.glob(os.path.join(img_dir, \"**\", \"*.png\"), recursive=True) +\n",
    "                  glob.glob(os.path.join(img_dir, \"**\", \"*.jpeg\"), recursive=True))\n",
    "    lbls = sorted(glob.glob(os.path.join(lbl_dir, \"**\", \"*.txt\"), recursive=True))\n",
    "    # pares por stem\n",
    "    img_stems = set([os.path.splitext(os.path.basename(p))[0] for p in imgs])\n",
    "    lbl_stems = set([os.path.splitext(os.path.basename(p))[0] for p in lbls])\n",
    "    missing_lbl = img_stems - lbl_stems\n",
    "    missing_img = lbl_stems - img_stems\n",
    "    return imgs, lbls, missing_lbl, missing_img\n",
    "\n",
    "print(\"== Checando YOLO splits ==\")\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    sd = os.path.join(YOLO_DIR, split)\n",
    "    assert os.path.isdir(sd), f\"Split {split} n√£o encontrado em {YOLO_DIR}\"\n",
    "    imgs, lbls, missing_lbl, missing_img = count_images_labels(sd)\n",
    "    print(f\"{split}: imgs={len(imgs)} lbls={len(lbls)}\")\n",
    "    assert len(imgs) > 0 and len(lbls) > 0, f\"Split {split} vazio.\"\n",
    "    assert len(missing_lbl) == 0, f\"{split}: imagens sem label: {list(sorted(missing_lbl))[:5]}...\"\n",
    "    assert len(missing_img) == 0, f\"{split}: labels sem imagem: {list(sorted(missing_img))[:5]}...\"\n",
    "\n",
    "print(\"\\\\n== Checando/Padronizando COCO ==\")\n",
    "ann_dir = os.path.join(COCO_DIR, \"annotations\")\n",
    "os.makedirs(ann_dir, exist_ok=True)\n",
    "\n",
    "def is_coco_json(p):\n",
    "    try:\n",
    "        with open(p, \"r\") as f: j = json.load(f)\n",
    "        return all(k in j for k in [\"images\",\"annotations\",\"categories\"])\n",
    "    except: return False\n",
    "\n",
    "# Se n√£o existirem instances_*.json, tentamos copiar de _annotations.coco.json nos splits\n",
    "required = {\n",
    "    \"train\": os.path.join(ann_dir, \"instances_train.json\"),\n",
    "    \"val\":   os.path.join(ann_dir, \"instances_val.json\"),\n",
    "    \"test\":  os.path.join(ann_dir, \"instances_test.json\"),\n",
    "}\n",
    "\n",
    "def try_copy_from_split(split_key, split_folder_name):\n",
    "    split_dir = os.path.join(COCO_DIR, split_folder_name)\n",
    "    cands = [\n",
    "        os.path.join(split_dir, \"_annotations.coco.json\"),\n",
    "        os.path.join(split_dir, \"annotation.json\"),\n",
    "        os.path.join(split_dir, \"anotation.json\"),\n",
    "        os.path.join(split_dir, \"annotations.json\"),\n",
    "        os.path.join(split_dir, f\"instances_{split_key}.json\"),\n",
    "    ]\n",
    "    for p in cands:\n",
    "        if os.path.exists(p) and is_coco_json(p):\n",
    "            shutil.copy2(p, required[split_key])\n",
    "            return True, p\n",
    "    return False, None\n",
    "\n",
    "# 'valid' como alias de 'val'\n",
    "split_map = {\"train\":\"train\", \"val\":\"val\" if os.path.isdir(os.path.join(COCO_DIR,\"val\")) else \"valid\", \"test\":\"test\"}\n",
    "\n",
    "for sk, dst in required.items():\n",
    "    if not os.path.exists(dst):\n",
    "        ok, src = try_copy_from_split(sk, split_map[sk])\n",
    "        if ok:\n",
    "            print(f\"[OK] {sk}: copiado {src} -> {dst}\")\n",
    "        else:\n",
    "            print(f\"[WARN] {sk}: n√£o encontrado JSON COCO; ser√° necess√°rio converter do YOLO se este split for usado.\")\n",
    "\n",
    "# Verifica presen√ßa final\n",
    "print()\n",
    "for name, path in [(\"instances_train\", required[\"train\"]), (\"instances_val\", required[\"val\"]), (\"instances_test\", required[\"test\"])]:\n",
    "    print(name, \"->\", os.path.exists(path), path)\n",
    "\n",
    "# Corrige data.yaml (caminhos absolutos + chave 'val')\n",
    "data_yaml_guess = os.path.join(YOLO_DIR, \"data.yaml\")\n",
    "assert os.path.exists(data_yaml_guess), f\"data.yaml n√£o encontrado em {YOLO_DIR}\"\n",
    "\n",
    "with open(data_yaml_guess, \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "if \"valid\" in data and \"val\" not in data:\n",
    "    data[\"val\"] = data.pop(\"valid\")\n",
    "\n",
    "def absolutize(p):\n",
    "    if p is None: return p\n",
    "    if isinstance(p, list):\n",
    "        return [absolutize(x) for x in p]\n",
    "    p = str(p)\n",
    "    if p.startswith(\"/\"):\n",
    "        return p\n",
    "    return os.path.normpath(os.path.join(YOLO_DIR, p))\n",
    "\n",
    "for key in [\"path\", \"train\", \"val\", \"test\"]:\n",
    "    if key in data:\n",
    "        data[key] = absolutize(data[key])\n",
    "\n",
    "data.pop(\"path\", None)\n",
    "\n",
    "os.makedirs(CONFIG[\"OUT_YOLO\"], exist_ok=True)\n",
    "fixed_yaml = os.path.join(CONFIG[\"OUT_YOLO\"], \"data_fixed.yaml\")\n",
    "with open(fixed_yaml, \"w\") as f:\n",
    "    yaml.safe_dump(data, f)\n",
    "print(\"\\\\nSalvo data.yaml corrigido em:\", fixed_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e91de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4) Instala√ß√£o de depend√™ncias e seeds (fix cu124 + vers√£o do pycocotools)\n",
    "import os, random, numpy as np\n",
    "\n",
    "need_cu124 = os.environ.get(\"TORCH_NEED_CU124\", \"0\") == \"1\"\n",
    "\n",
    "if need_cu124:\n",
    "    print(\">> Ajustando PyTorch para cu124 (driver CUDA 12.4 detectado) ‚Üí torch 2.6.0 + torchvision 0.21.0\")\n",
    "    # Instala vers√µes compat√≠veis com cu124 dispon√≠veis oficialmente\n",
    "    !pip install -q --upgrade --force-reinstall         torch==2.6.0+cu124 torchvision==0.21.0+cu124         --index-url https://download.pytorch.org/whl/cu124\n",
    "else:\n",
    "    print(\"Mantendo PyTorch atual.\")\n",
    "\n",
    "# Demais depend√™ncias (idempotente)\n",
    "!pip install -q -U ultralytics pycocotools transformers datasets evaluate accelerate\n",
    "\n",
    "# Imports p√≥s-instala√ß√£o\n",
    "import torch, torchvision, transformers, datasets, evaluate, ultralytics\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "try:\n",
    "    coco_ver = version(\"pycocotools\")\n",
    "except PackageNotFoundError:\n",
    "    coco_ver = \"not installed\"\n",
    "\n",
    "print({\n",
    "    \"torch\": torch.__version__,\n",
    "    \"torchvision\": torchvision.__version__,\n",
    "    \"torch_cuda_runtime\": torch.version.cuda,\n",
    "    \"ultralytics\": ultralytics.__version__,\n",
    "    \"transformers\": transformers.__version__,\n",
    "    \"datasets\": datasets.__version__,\n",
    "    \"evaluate\": evaluate.__version__,\n",
    "    \"pycocotools\": coco_ver,\n",
    "})\n",
    "\n",
    "# Seeds determin√≠sticas\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(CONFIG[\"SEED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5) Treino YOLO-seg (v12‚Üív11‚Üív8) + Valida√ß√£o + Amostras\n",
    "import os, time, json, glob\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "\n",
    "os.makedirs(CONFIG[\"OUT_YOLO\"], exist_ok=True)\n",
    "\n",
    "ckpts = [\n",
    "    f\"yolo12{CONFIG['MODEL_SIZE']}-seg.pt\",\n",
    "    f\"yolo11{CONFIG['MODEL_SIZE']}-seg.pt\",\n",
    "    f\"yolov8{CONFIG['MODEL_SIZE']}-seg.pt\",\n",
    "]\n",
    "\n",
    "chosen_ckpt = None\n",
    "model = None\n",
    "errors = []\n",
    "\n",
    "for ck in ckpts:\n",
    "    try:\n",
    "        print(f\"Tentando carregar: {ck}\")\n",
    "        model = YOLO(ck)\n",
    "        chosen_ckpt = ck\n",
    "        print(\"Carregado:\", ck)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Falhou: {ck} -> {e}\")\n",
    "        errors.append((ck, str(e)))\n",
    "\n",
    "assert model is not None, f\"N√£o foi poss√≠vel carregar nenhum checkpoint. Erros: {errors}\"\n",
    "\n",
    "RUN_NAME = f\"yolo_seg_{os.path.splitext(os.path.basename(chosen_ckpt))[0]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "results = model.train(\n",
    "    data=os.path.join(CONFIG[\"OUT_YOLO\"], \"data_fixed.yaml\"),\n",
    "    imgsz=CONFIG[\"IMG_SIZE\"],\n",
    "    epochs=CONFIG[\"EPOCHS\"],\n",
    "    batch=CONFIG[\"BATCH\"],\n",
    "    device=0,\n",
    "    workers=CONFIG[\"YOLO_WORKERS\"],\n",
    "    pretrained=True,\n",
    "    patience=CONFIG[\"YOLO_PATIENCE\"],\n",
    "    rect=True,\n",
    "    project=CONFIG[\"OUT_YOLO\"],\n",
    "    name=RUN_NAME,\n",
    "    seed=CONFIG[\"SEED\"],\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "val_res = model.val(\n",
    "    data=os.path.join(CONFIG[\"OUT_YOLO\"], \"data_fixed.yaml\"),\n",
    "    imgsz=CONFIG[\"IMG_SIZE\"],\n",
    "    device=0,\n",
    "    split=\"val\",\n",
    "    conf=CONFIG[\"CONF\"],\n",
    "    iou=CONFIG[\"IOU_THRES\"],\n",
    "    project=CONFIG[\"OUT_YOLO\"],\n",
    "    name=RUN_NAME+\"_val\",\n",
    "    save_json=True,\n",
    ")\n",
    "print(\"YOLO val metrics:\", getattr(val_res, \"results_dict\", val_res))\n",
    "\n",
    "# Predi√ß√µes de amostra (primeiras 8 imagens do split val)\n",
    "import yaml, glob, os\n",
    "with open(os.path.join(CONFIG[\"OUT_YOLO\"], \"data_fixed.yaml\"), \"r\") as f:\n",
    "    d_yaml = yaml.safe_load(f)\n",
    "\n",
    "val_img_dir = os.path.join(d_yaml[\"val\"], \"images\") if isinstance(d_yaml[\"val\"], str) else d_yaml[\"val\"]\n",
    "sample_imgs = sorted(glob.glob(os.path.join(val_img_dir, \"**\", \"*.jpg\"), recursive=True))[:8]\n",
    "pred_out_dir = os.path.join(CONFIG[\"OUT_YOLO\"], RUN_NAME, \"sample_preds\")\n",
    "os.makedirs(pred_out_dir, exist_ok=True)\n",
    "\n",
    "preds = model.predict(\n",
    "    source=sample_imgs,\n",
    "    conf=CONFIG[\"CONF\"],\n",
    "    iou=CONFIG[\"IOU_THRES\"],\n",
    "    imgsz=CONFIG[\"IMG_SIZE\"],\n",
    "    device=0,\n",
    "    save=True,\n",
    "    project=pred_out_dir,\n",
    "    name=\"yolo_samples\",\n",
    "    exist_ok=True,\n",
    ")\n",
    "print(\"Amostras YOLO salvas em:\", os.path.join(pred_out_dir, \"yolo_samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdb41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 6) RT-DETR-Seg (Transformers) ‚Äî Dataset COCO + Treino + Avalia√ß√£o\n",
    "import os, json, math, time, numpy as np, torch\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as mask_util\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(CONFIG[\"OUT_RTDETR\"], exist_ok=True)\n",
    "\n",
    "# === Dataset COCO para instance segmentation ===\n",
    "class COCOSegDataset(Dataset):\n",
    "    def __init__(self, img_root, ann_json, processor, img_size=1024):\n",
    "        self.coco = COCO(ann_json)\n",
    "        self.img_root = img_root\n",
    "        self.processor = processor\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # filtra imagens com ao menos uma anota√ß√£o\n",
    "        keep = []\n",
    "        for img_id in self.img_ids:\n",
    "            anns = self.coco.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "            if len(anns) > 0:\n",
    "                keep.append(img_id)\n",
    "        self.img_ids = keep\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        info = self.coco.loadImgs([img_id])[0]\n",
    "        file_name = info[\"file_name\"]\n",
    "        path = os.path.join(self.img_root, file_name)\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        anns = [a for a in anns if \"bbox\" in a and \"category_id\" in a and \"segmentation\" in a and a.get(\"iscrowd\",0) == 0]\n",
    "\n",
    "        encoded = self.processor(\n",
    "            images=image,\n",
    "            annotations={\"image_id\": img_id, \"annotations\": anns},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # remove dim de batch artificial\n",
    "        for k in encoded:\n",
    "            if isinstance(encoded[k], torch.Tensor):\n",
    "                encoded[k] = encoded[k].squeeze(0)\n",
    "            elif isinstance(encoded[k], list) and len(encoded[k]) == 1:\n",
    "                encoded[k] = encoded[k][0]\n",
    "        encoded[\"image_id\"] = img_id\n",
    "        encoded[\"orig_size\"] = (image.height, image.width)\n",
    "        return encoded\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([b[\"pixel_values\"] for b in batch])\n",
    "    pixel_mask = torch.stack([b[\"pixel_mask\"] for b in batch]) if \"pixel_mask\" in batch[0] else None\n",
    "    labels = [b[\"labels\"] for b in batch]\n",
    "    image_ids = [b[\"image_id\"] for b in batch]\n",
    "    orig_sizes = [b[\"orig_size\"] for b in batch]\n",
    "    res = {\"pixel_values\": pixel_values, \"labels\": labels, \"image_ids\": image_ids, \"orig_sizes\": orig_sizes}\n",
    "    if pixel_mask is not None:\n",
    "        res[\"pixel_mask\"] = pixel_mask\n",
    "    return res\n",
    "\n",
    "# === Escolha do modelo/processador (RT-DETR-Seg ‚Üí fallback Mask2Former) ===\n",
    "from transformers import AutoImageProcessor, AutoModelForInstanceSegmentation\n",
    "\n",
    "chosen_transformer = None\n",
    "chosen_processor = None\n",
    "model_candidates = [\n",
    "    # tentativa ideal (pode n√£o existir no HF)\n",
    "    \"PaddlePaddle/rt-detr-hf-l-seg\",\n",
    "    # alternativas Mask2Former (instance)\n",
    "    \"facebook/mask2former-swin-large-coco-instance\",\n",
    "    \"facebook/mask2former-swin-base-coco-instance\",\n",
    "    \"facebook/mask2former-swin-small-coco-instance\",\n",
    "]\n",
    "\n",
    "load_errors = []\n",
    "for name in model_candidates:\n",
    "    try:\n",
    "        print(f\"Tentando carregar: {name}\")\n",
    "        chosen_processor = AutoImageProcessor.from_pretrained(name)\n",
    "        chosen_transformer = AutoModelForInstanceSegmentation.from_pretrained(name)\n",
    "        print(\"Carregado:\", name)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Falhou: {name} -> {e}\")\n",
    "        load_errors.append((name, str(e)))\n",
    "\n",
    "assert chosen_transformer is not None, f\"N√£o foi poss√≠vel carregar modelo Transformer de instance-seg. Erros: {load_errors}\"\n",
    "chosen_transformer.to(device)\n",
    "\n",
    "# === Datasets/DataLoaders (COCO) ===\n",
    "train_img_dir = os.path.join(CONFIG[\"COCO_DIR\"], \"train\")\n",
    "val_img_dir   = os.path.join(CONFIG[\"COCO_DIR\"], \"val\") if os.path.isdir(os.path.join(CONFIG[\"COCO_DIR\"], \"val\")) else os.path.join(CONFIG[\"COCO_DIR\"], \"valid\")\n",
    "ann_dir = os.path.join(CONFIG[\"COCO_DIR\"], \"annotations\")\n",
    "\n",
    "train_json = os.path.join(ann_dir, \"instances_train.json\")\n",
    "val_json   = os.path.join(ann_dir, \"instances_val.json\")\n",
    "assert os.path.exists(train_json) and os.path.exists(val_json), \"JSONs COCO train/val necess√°rios.\"\n",
    "\n",
    "train_ds = COCOSegDataset(train_img_dir, train_json, chosen_processor, CONFIG[\"IMG_SIZE\"])\n",
    "val_ds   = COCOSegDataset(val_img_dir,   val_json,   chosen_processor, CONFIG[\"IMG_SIZE\"])\n",
    "\n",
    "rt_batch = max(1, min(2, CONFIG[\"BATCH\"]))  # batch pequeno por VRAM\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=rt_batch, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=1,       shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "# === Otimizador e agendamento ===\n",
    "from torch.optim import AdamW\n",
    "total_steps = math.ceil(len(train_loader) * CONFIG[\"EPOCHS\"] / CONFIG[\"RT_GRAD_ACCUM\"])\n",
    "optimizer = AdamW(chosen_transformer.parameters(), lr=CONFIG[\"RT_LR\"], weight_decay=CONFIG[\"RT_WEIGHT_DECAY\"])\n",
    "\n",
    "def lr_lambda(step):\n",
    "    w = CONFIG[\"RT_WARMUP_STEPS\"]\n",
    "    if step < w:\n",
    "        return float(step) / max(1, w)\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=CONFIG[\"RT_USE_AMP\"])\n",
    "\n",
    "# === Loop de treino ===\n",
    "chosen_transformer.train()\n",
    "global_step = 0\n",
    "print(f\"Iniciando treino Transformers: epochs={CONFIG['EPOCHS']} batch={rt_batch} grad_accum={CONFIG['RT_GRAD_ACCUM']}\")\n",
    "\n",
    "for epoch in range(CONFIG[\"EPOCHS\"]):\n",
    "    pbar = tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CONFIG['EPOCHS']}]\")\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = [{k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in t.items()} for t in batch[\"labels\"]]\n",
    "        pixel_mask = batch.get(\"pixel_mask\")\n",
    "        if pixel_mask is not None:\n",
    "            pixel_mask = pixel_mask.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CONFIG[\"RT_USE_AMP\"]):\n",
    "            outputs = chosen_transformer(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "            loss = outputs.loss / CONFIG[\"RT_GRAD_ACCUM\"]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % CONFIG[\"RT_GRAD_ACCUM\"] == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "        pbar.set_postfix({\"loss\": float(loss.item())})\n",
    "\n",
    "# === Avalia√ß√£o COCO (bbox e segm) ===\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "chosen_transformer.eval()\n",
    "coco_gt = COCO(val_json)\n",
    "bbox_dets, segm_dets = [], []\n",
    "infer_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Infer√™ncia valida√ß√£o\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        pixel_mask = batch.get(\"pixel_mask\")\n",
    "        if pixel_mask is not None:\n",
    "            pixel_mask = pixel_mask.to(device)\n",
    "        image_ids = batch[\"image_ids\"]\n",
    "        orig_sizes = batch[\"orig_sizes\"]  # (H, W)\n",
    "\n",
    "        start = time.time()\n",
    "        outputs = chosen_transformer(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        infer_times.append((time.time() - start) * 1000.0)  # ms\n",
    "\n",
    "        try:\n",
    "            post = chosen_processor.post_process_instance_segmentation(\n",
    "                outputs, threshold=CONFIG[\"RT_EVAL_CONF\"], target_sizes=[orig_sizes[0]]\n",
    "            )[0]\n",
    "            scores = post[\"scores\"].cpu().numpy().tolist()\n",
    "            labels = post[\"labels\"].cpu().numpy().tolist()\n",
    "            boxes  = post[\"boxes\"].cpu().numpy().tolist()\n",
    "            masks  = post[\"masks\"].cpu().numpy()\n",
    "            img_id = int(image_ids[0])\n",
    "            for s, c, b, m in zip(scores, labels, boxes, masks):\n",
    "                x1,y1,x2,y2 = b\n",
    "                w = max(0.0, x2 - x1); h = max(0.0, y2 - y1)\n",
    "                coco_bbox = [float(x1), float(y1), float(w), float(h)]\n",
    "                bbox_dets.append({\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": int(c),\n",
    "                    \"bbox\": coco_bbox,\n",
    "                    \"score\": float(s),\n",
    "                })\n",
    "                m_bin = (m > 0.5).astype(np.uint8)\n",
    "                rle = mask_util.encode(np.asfortranarray(m_bin))\n",
    "                rle[\"counts\"] = rle[\"counts\"].decode(\"ascii\")\n",
    "                segm_dets.append({\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": int(c),\n",
    "                    \"segmentation\": rle,\n",
    "                    \"score\": float(s),\n",
    "                })\n",
    "        except Exception:\n",
    "            post = chosen_processor.post_process_object_detection(\n",
    "                outputs, threshold=CONFIG[\"RT_EVAL_CONF\"], target_sizes=[orig_sizes[0]]\n",
    "            )[0]\n",
    "            scores = post[\"scores\"].cpu().numpy().tolist()\n",
    "            labels = post[\"labels\"].cpu().numpy().tolist()\n",
    "            boxes  = post[\"boxes\"].cpu().numpy().tolist()\n",
    "            img_id = int(image_ids[0])\n",
    "            for s, c, b in zip(scores, labels, boxes):\n",
    "                x1,y1,x2,y2 = b\n",
    "                w = max(0.0, x2 - x1); h = max(0.0, y2 - y1)\n",
    "                coco_bbox = [float(x1), float(y1), float(w), float(h)]\n",
    "                bbox_dets.append({\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": int(c),\n",
    "                    \"bbox\": coco_bbox,\n",
    "                    \"score\": float(s),\n",
    "                })\n",
    "\n",
    "# Salva resultados\n",
    "from datetime import datetime\n",
    "rt_out_dir = os.path.join(CONFIG[\"OUT_RTDETR\"], \"preds_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "os.makedirs(rt_out_dir, exist_ok=True)\n",
    "bbox_json = os.path.join(rt_out_dir, \"bbox_results.json\")\n",
    "segm_json = os.path.join(rt_out_dir, \"segm_results.json\")\n",
    "with open(bbox_json, \"w\") as f: json.dump(bbox_dets, f)\n",
    "with open(segm_json, \"w\") as f: json.dump(segm_dets, f)\n",
    "print(\"Salvos:\", bbox_json, segm_json)\n",
    "\n",
    "# COCOeval\n",
    "coco_dt_bbox = coco_gt.loadRes(bbox_json) if len(bbox_dets)>0 else None\n",
    "bbox_metrics = {}\n",
    "if coco_dt_bbox is not None:\n",
    "    coco_eval_bbox = COCOeval(coco_gt, coco_dt_bbox, iouType=\"bbox\")\n",
    "    coco_eval_bbox.evaluate(); coco_eval_bbox.accumulate(); coco_eval_bbox.summarize()\n",
    "    bbox_metrics = {\n",
    "        \"AP@[.5:.95]_bbox\": float(coco_eval_bbox.stats[0]),\n",
    "        \"AP50_bbox\": float(coco_eval_bbox.stats[1]),\n",
    "        \"AP75_bbox\": float(coco_eval_bbox.stats[2]),\n",
    "    }\n",
    "\n",
    "segm_metrics = {}\n",
    "if len(segm_dets) > 0:\n",
    "    coco_dt_segm = coco_gt.loadRes(segm_json)\n",
    "    coco_eval_segm = COCOeval(coco_gt, coco_dt_segm, iouType=\"segm\")\n",
    "    coco_eval_segm.evaluate(); coco_eval_segm.accumulate(); coco_eval_segm.summarize()\n",
    "    segm_metrics = {\n",
    "        \"AP@[.5:.95]_segm\": float(coco_eval_segm.stats[0]),\n",
    "        \"AP50_segm\": float(coco_eval_segm.stats[1]),\n",
    "        \"AP75_segm\": float(coco_eval_segm.stats[2]),\n",
    "    }\n",
    "\n",
    "rt_infer_ms = float(np.mean(infer_times)) if infer_times else None\n",
    "print(\"Transformer bbox metrics:\", bbox_metrics)\n",
    "print(\"Transformer segm metrics:\", segm_metrics)\n",
    "print(\"Transformer infer time (ms/img):\", rt_infer_ms)\n",
    "\n",
    "# Amostras salvas\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "sample_save = os.path.join(rt_out_dir, \"samples\")\n",
    "os.makedirs(sample_save, exist_ok=True)\n",
    "\n",
    "val_ids = val_ds.img_ids[:6]\n",
    "for img_id in val_ids:\n",
    "    info = val_ds.coco.loadImgs([img_id])[0]\n",
    "    path = os.path.join(val_img_dir, info[\"file_name\"])\n",
    "    img = np.array(Image.open(path).convert(\"RGB\"))\n",
    "\n",
    "    bds = [d for d in bbox_dets if d[\"image_id\"] == img_id and d[\"score\"] >= CONFIG[\"CONF\"]]\n",
    "    sds = [d for d in segm_dets if d[\"image_id\"] == img_id and d[\"score\"] >= CONFIG[\"CONF\"]]\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(img)\n",
    "    for d in bds:\n",
    "        x,y,w,h = d[\"bbox\"]\n",
    "        ax.add_patch(Rectangle((x,y), w, h, fill=False, linewidth=1.5))\n",
    "    for d in sds[:20]:\n",
    "        m = mask_util.decode(d[\"segmentation\"])\n",
    "        ax.contour(m, levels=[0.5], linewidths=1.0)\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(sample_save, f\"{img_id}.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Amostras de RT/Mask2Former salvas em:\", sample_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 7) Coleta de m√©tricas + compara√ß√£o (DataFrame + CSV)\n",
    "import os, json, pandas as pd, torch, time, numpy as np, glob, yaml\n",
    "\n",
    "# YOLO metrics\n",
    "yolo_run_dir = os.path.join(CONFIG[\"OUT_YOLO\"], RUN_NAME)\n",
    "yolo_results_json = os.path.join(yolo_run_dir, \"results.json\")\n",
    "yolo_metrics = {}\n",
    "if os.path.exists(yolo_results_json):\n",
    "    with open(yolo_results_json, \"r\") as f:\n",
    "        yolo_metrics = json.load(f)\n",
    "else:\n",
    "    try:\n",
    "        yolo_metrics = getattr(val_res, \"results_dict\", {})\n",
    "    except:\n",
    "        yolo_metrics = {}\n",
    "\n",
    "def safe_get(d, keys, default=None):\n",
    "    for k in keys:\n",
    "        if k in d: return d[k]\n",
    "    return default\n",
    "\n",
    "yolo_map50      = safe_get(yolo_metrics, [\"metrics/precision(B)\",\"metrics/mAP50(B)\",\"metrics/mAP50(M)\",\"map50\",\"mAP50\"], None)\n",
    "yolo_map5095_b  = safe_get(yolo_metrics, [\"metrics/mAP50-95(B)\",\"map\",\"mAP50-95(B)\"], None)\n",
    "yolo_map5095_m  = safe_get(yolo_metrics, [\"metrics/mAP50-95(M)\",\"map_mask\",\"mAP50-95(M)\"], None)\n",
    "\n",
    "# par√¢metros YOLO\n",
    "yolo_params = None\n",
    "try:\n",
    "    yolo_params = sum(p.numel() for p in model.model.parameters()) / 1e6\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# tempo de infer√™ncia YOLO (mediana em ~8 imgs)\n",
    "with open(os.path.join(CONFIG[\"OUT_YOLO\"], \"data_fixed.yaml\"), \"r\") as f:\n",
    "    d_yaml = yaml.safe_load(f)\n",
    "imgs_inf = sorted(glob.glob(os.path.join(d_yaml[\"val\"], \"images\", \"**\", \"*.jpg\"), recursive=True))[:8]\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    for p in imgs_inf:\n",
    "        s = time.time()\n",
    "        _ = model.predict(source=p, imgsz=CONFIG[\"IMG_SIZE\"], conf=CONFIG[\"CONF\"], iou=CONFIG[\"IOU_THRES\"], device=0, verbose=False)\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        times.append((time.time()-s)*1000.0)\n",
    "yolo_infer_ms = float(np.median(times)) if times else None\n",
    "\n",
    "# Transformer metrics (do passo anterior)\n",
    "rt_bbox_ap   = bbox_metrics.get(\"AP@[.5:.95]_bbox\")\n",
    "rt_segm_ap   = segm_metrics.get(\"AP@[.5:.95]_segm\")\n",
    "rt_ap50_bbox = bbox_metrics.get(\"AP50_bbox\")\n",
    "rt_ap50_segm = segm_metrics.get(\"AP50_segm\")\n",
    "\n",
    "rt_params = sum(p.numel() for p in chosen_transformer.parameters()) / 1e6\n",
    "rt_gflops = None\n",
    "rt_infer = rt_infer_ms\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": f\"YOLO{chosen_ckpt.replace('.pt','')}\",\n",
    "        \"imgsz\": CONFIG[\"IMG_SIZE\"],\n",
    "        \"epochs\": CONFIG[\"EPOCHS\"],\n",
    "        \"params(M)\": round(yolo_params, 2) if yolo_params else None,\n",
    "        \"GFLOPs\": None,\n",
    "        \"mAP50\": round(yolo_map50, 4) if yolo_map50 is not None else None,\n",
    "        \"mAP50-95 (bbox)\": round(yolo_map5095_b, 4) if yolo_map5095_b is not None else None,\n",
    "        \"mAP50-95 (mask)\": round(yolo_map5095_m, 4) if yolo_map5095_m is not None else None,\n",
    "        \"infer_time_ms/img\": round(yolo_infer_ms, 2) if yolo_infer_ms is not None else None\n",
    "    },\n",
    "    {\n",
    "        \"model\": getattr(chosen_transformer.config, \"_name_or_path\", \"transformer_instance_seg\"),\n",
    "        \"imgsz\": CONFIG[\"IMG_SIZE\"],\n",
    "        \"epochs\": CONFIG[\"EPOCHS\"],\n",
    "        \"params(M)\": round(rt_params, 2),\n",
    "        \"GFLOPs\": rt_gflops,\n",
    "        \"mAP50\": round(rt_ap50_bbox, 4) if rt_ap50_bbox is not None else None,\n",
    "        \"mAP50-95 (bbox)\": round(rt_bbox_ap, 4) if rt_bbox_ap is not None else None,\n",
    "        \"mAP50-95 (mask)\": round(rt_segm_ap, 4) if rt_segm_ap is not None else None,\n",
    "        \"infer_time_ms/img\": round(rt_infer, 2) if rt_infer is not None else None\n",
    "    }\n",
    "])\n",
    "\n",
    "cmp_csv = os.path.join(CONFIG[\"OUT_RTDETR\"], \"comparacao_metrics.csv\")\n",
    "os.makedirs(CONFIG[\"OUT_RTDETR\"], exist_ok=True)\n",
    "df.to_csv(cmp_csv, index=False)\n",
    "print(\"CSV salvo em:\", cmp_csv)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 8) Visualiza√ß√µes lado a lado (YOLO vs RT/Mask2Former) no split de valida√ß√£o\n",
    "import os, glob, numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "with open(os.path.join(CONFIG[\"OUT_YOLO\"], \"data_fixed.yaml\"), \"r\") as f:\n",
    "    d_yaml = yaml.safe_load(f)\n",
    "\n",
    "val_imgs = sorted(glob.glob(os.path.join(d_yaml[\"val\"], \"images\", \"**\", \"*.jpg\"), recursive=True))[:6]\n",
    "side_by_side_dir = os.path.join(CONFIG[\"OUT_RTDETR\"], \"side_by_side\")\n",
    "os.makedirs(side_by_side_dir, exist_ok=True)\n",
    "\n",
    "def yolo_predict_on_image(img_path):\n",
    "    res = model.predict(source=img_path, imgsz=CONFIG[\"IMG_SIZE\"], conf=CONFIG[\"CONF\"], iou=CONFIG[\"IOU_THRES\"], device=0, verbose=False)\n",
    "    boxes, masks, scores = [], [], []\n",
    "    if len(res) > 0 and hasattr(res[0], \"boxes\"):\n",
    "        b = res[0].boxes\n",
    "        if b is not None:\n",
    "            for i in range(len(b)):\n",
    "                boxes.append(b.xyxy[i].cpu().numpy().tolist())\n",
    "                scores.append(float(b.conf[i].item()))\n",
    "    if len(res) > 0 and getattr(res[0], \"masks\", None) is not None and res[0].masks is not None:\n",
    "        for m in res[0].masks.data.cpu().numpy():\n",
    "            masks.append(m)\n",
    "    return boxes, masks, scores\n",
    "\n",
    "def rt_predict_on_image(img_path):\n",
    "    im = Image.open(img_path).convert(\"RGB\")\n",
    "    enc = chosen_processor(images=im, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = chosen_transformer(**enc)\n",
    "    H, W = im.height, im.width\n",
    "    try:\n",
    "        post = chosen_processor.post_process_instance_segmentation(out, threshold=CONFIG[\"CONF\"], target_sizes=[(H,W)])[0]\n",
    "        boxes = post[\"boxes\"].cpu().numpy().tolist()\n",
    "        scores = post[\"scores\"].cpu().numpy().tolist()\n",
    "        masks  = post[\"masks\"].cpu().numpy()\n",
    "    except Exception:\n",
    "        post = chosen_processor.post_process_object_detection(out, threshold=CONFIG[\"CONF\"], target_sizes=[(H,W)])[0]\n",
    "        boxes = post[\"boxes\"].cpu().numpy().tolist()\n",
    "        scores = post[\"scores\"].cpu().numpy().tolist()\n",
    "        masks  = []\n",
    "    return boxes, masks, scores\n",
    "\n",
    "for p in val_imgs:\n",
    "    img = np.array(Image.open(p).convert(\"RGB\"))\n",
    "    y_boxes, y_masks, y_scores = yolo_predict_on_image(p)\n",
    "    r_boxes, r_masks, r_scores = rt_predict_on_image(p)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    axes[0].imshow(img); axes[0].set_title(\"YOLO-seg\")\n",
    "    for bb in y_boxes:\n",
    "        x1,y1,x2,y2 = bb\n",
    "        axes[0].add_patch(Rectangle((x1,y1), x2-x1, y2-y1, fill=False, linewidth=1.5))\n",
    "    for m in y_masks[:20]:\n",
    "        axes[0].contour(m, levels=[0.5], linewidths=1.0)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(img); axes[1].set_title(\"RT-DETR-Seg (fallback Mask2Former se aplic√°vel)\")\n",
    "    for bb in r_boxes:\n",
    "        x1,y1,x2,y2 = bb\n",
    "        axes[1].add_patch(Rectangle((x1,y1), x2-x1, y2-y1, fill=False, linewidth=1.5))\n",
    "    for m in r_masks[:20]:\n",
    "        axes[1].contour((m>0.5).astype(np.uint8), levels=[0.5], linewidths=1.0)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    import os\n",
    "    plt.tight_layout()\n",
    "    base = os.path.splitext(os.path.basename(p))[0]\n",
    "    outp = os.path.join(side_by_side_dir, f\"{base}_compare.png\")\n",
    "    plt.savefig(outp, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Visualiza√ß√µes lado a lado salvas em:\", side_by_side_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 9) Export√°veis (YOLO ‚Üí ONNX) e compacta√ß√£o de runs\n",
    "import os, shutil, zipfile\n",
    "from ultralytics import YOLO\n",
    "\n",
    "onnx_path = None\n",
    "try:\n",
    "    exp = model.export(format=\"onnx\", imgsz=CONFIG[\"IMG_SIZE\"], opset=12, dynamic=False, simplify=False)\n",
    "    onnx_path = exp\n",
    "    print(\"Modelo YOLO exportado para ONNX:\", onnx_path)\n",
    "except Exception as e:\n",
    "    print(\"Falha ao exportar ONNX:\", e)\n",
    "\n",
    "def zip_dir(src_dir, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, _, files in os.walk(src_dir):\n",
    "            for f in files:\n",
    "                fp = os.path.join(root, f)\n",
    "                arc = os.path.relpath(fp, src_dir)\n",
    "                zf.write(fp, arc)\n",
    "\n",
    "yolo_zip = os.path.join(CONFIG[\"OUT_YOLO\"], f\"{RUN_NAME}.zip\")\n",
    "try:\n",
    "    zip_dir(os.path.join(CONFIG[\"OUT_YOLO\"], RUN_NAME), yolo_zip)\n",
    "    print(\"Zip YOLO salvo em:\", yolo_zip)\n",
    "except Exception as e:\n",
    "    print(\"Zip YOLO falhou:\", e)\n",
    "\n",
    "rt_zip = os.path.join(CONFIG[\"OUT_RTDETR\"], \"transformer_run.zip\")\n",
    "try:\n",
    "    zip_dir(os.path.join(CONFIG[\"OUT_RTDETR\"]), rt_zip)\n",
    "    print(\"Zip Transformers salvo em:\", rt_zip)\n",
    "except Exception as e:\n",
    "    print(\"Zip Transformers falhou:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf01187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 10) Reprodutibilidade: salvar report.json\n",
    "import json, os, platform, torch, transformers, ultralytics, datasets, evaluate, torchvision\n",
    "report = {\n",
    "    \"config\": CONFIG,\n",
    "    \"env\": {\n",
    "        \"python\": platform.python_version(),\n",
    "        \"torch\": torch.__version__,\n",
    "        \"torchvision\": torchvision.__version__,\n",
    "        \"transformers\": transformers.__version__,\n",
    "        \"ultralytics\": ultralytics.__version__,\n",
    "        \"datasets\": datasets.__version__,\n",
    "        \"evaluate\": evaluate.__version__,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "        \"torch_cuda_runtime\": torch.version.cuda,\n",
    "    },\n",
    "    \"yolo\": {\n",
    "        \"checkpoint\": chosen_ckpt,\n",
    "        \"run_dir\": os.path.join(CONFIG[\"OUT_YOLO\"], RUN_NAME),\n",
    "        \"inference_ms\": None,  # preenchido na tabela\n",
    "    },\n",
    "    \"transformer\": {\n",
    "        \"name\": getattr(chosen_transformer.config, \"_name_or_path\", None),\n",
    "    },\n",
    "    \"comparacao_csv\": os.path.join(CONFIG[\"OUT_RTDETR\"], \"comparacao_metrics.csv\"),\n",
    "}\n",
    "report_path = os.path.join(CONFIG[\"OUT_RTDETR\"], \"report.json\")\n",
    "os.makedirs(CONFIG[\"OUT_RTDETR\"], exist_ok=True)\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(\"Report salvo em:\", report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08eed1b",
   "metadata": {},
   "source": [
    "\n",
    "## ‚ÑπÔ∏è Notas / Como ajustar\n",
    "\n",
    "- **Por que `IMG_SIZE=1024`?** Radiografias t√™m estruturas finas; 1024 preserva detalhe sem estourar VRAM na T4 (com batch moderado).\n",
    "- **`rect=True` no YOLO:** batching retangular reduz padding e preserva propor√ß√µes.\n",
    "- **Ajustes comuns:** `BATCH` (VRAM), `EPOCHS` (treino mais longo), `MODEL_SIZE` (`\"x\"` para mais capacidade; `\"m\"/\"s\"` para menos VRAM), `patience` (early stopping do YOLO).\n",
    "- **Transformers (RT/Mask2Former):** manter `batch=1‚Äì2`, usar `gradient_accumulation` e AMP (`autocast`) para caber na T4.\n",
    "- **Fallback RT-DETR-Seg:** se n√£o existir checkpoint RT-DETR com cabe√ßa de m√°scara no HF, o notebook usa **Mask2Former** (*transformer + mask head*). O nome carregado aparece no log.\n",
    "- **COCO vs YOLO:** YOLO treina no **YOLO-format**; o pipeline Transformers usa o **COCO** **instance segmentation**. Esta notebook centraliza/normaliza `instances_{train,val,test}.json` em `COCO_DIR/annotations` e tenta copiar de `_annotations.coco.json` dos splits, se necess√°rio.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Comparacao_YOLO_RTDETR_Seg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
